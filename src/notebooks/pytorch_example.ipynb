{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch MLP example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will show an example of how to implement an MLP in PyTorch. Firstly, import PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the training data with `torch.tensor`. Most often you have to use `.float()` to avoid error messages when training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for XOR\n",
    "x = torch.tensor([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]]).float()\n",
    "t = torch.tensor([[0],\n",
    "                  [1],\n",
    "                  [1],\n",
    "                  [0]]).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A network structure is created by defining a class which extends the `torch.nn.Module` class. In the initialization, the layers should be defined. The linear layer is the one you are familiar with from previous exercises. It can be initialized with `torch.nn.Linear(n_inputs, n_units)`. Here, we create a network with a single hidden layer. The `forward` function defines how the forward propagation, that is how to compute the network output given the input and layers. PyTorch will automatically derive the gradients for backpropagation using this forward definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define neural network\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)\n",
    "        self.predict = torch.nn.Linear(n_hidden, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.hidden(x))\n",
    "        x = self.predict(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can instantiate our model using the class defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(n_feature=2, n_hidden=4, n_output=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the loss function to use, in this case the Mean Square Error (MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have to define the optimizer. We will use the stochastic gradient descent as in our own implementation, with learning rate 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a loop to train the network. This consists of 5 steps, each explained in the comments below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.26167148\n",
      "Loss:  0.25482735\n",
      "Loss:  0.25218737\n",
      "Loss:  0.25116548\n",
      "Loss:  0.25076747\n",
      "Loss:  0.25061008\n",
      "Loss:  0.25054568\n",
      "Loss:  0.25051713\n",
      "Loss:  0.2505025\n",
      "Loss:  0.25049326\n",
      "Loss:  0.25048608\n",
      "Loss:  0.25047976\n",
      "Loss:  0.2504737\n",
      "Loss:  0.25046784\n",
      "Loss:  0.25046209\n",
      "Loss:  0.2504563\n",
      "Loss:  0.2504506\n",
      "Loss:  0.25044492\n",
      "Loss:  0.25043923\n",
      "Loss:  0.2504336\n",
      "Loss:  0.25042802\n",
      "Loss:  0.25042242\n",
      "Loss:  0.25041682\n",
      "Loss:  0.25041133\n",
      "Loss:  0.2504058\n",
      "Loss:  0.25040027\n",
      "Loss:  0.25039485\n",
      "Loss:  0.25038937\n",
      "Loss:  0.25038397\n",
      "Loss:  0.25037855\n",
      "Loss:  0.25037318\n",
      "Loss:  0.25036782\n",
      "Loss:  0.25036246\n",
      "Loss:  0.25035715\n",
      "Loss:  0.25035188\n",
      "Loss:  0.25034657\n",
      "Loss:  0.25034133\n",
      "Loss:  0.2503361\n",
      "Loss:  0.2503309\n",
      "Loss:  0.25032568\n",
      "Loss:  0.25032052\n",
      "Loss:  0.2503154\n",
      "Loss:  0.25031027\n",
      "Loss:  0.25030518\n",
      "Loss:  0.25030008\n",
      "Loss:  0.250295\n",
      "Loss:  0.25028995\n",
      "Loss:  0.25028494\n",
      "Loss:  0.25027993\n",
      "Loss:  0.25027496\n",
      "Loss:  0.25027\n",
      "Loss:  0.250265\n",
      "Loss:  0.25026006\n",
      "Loss:  0.25025517\n",
      "Loss:  0.25025028\n",
      "Loss:  0.2502454\n",
      "Loss:  0.25024056\n",
      "Loss:  0.25023574\n",
      "Loss:  0.25023088\n",
      "Loss:  0.2502261\n",
      "Loss:  0.2502213\n",
      "Loss:  0.25021654\n",
      "Loss:  0.25021178\n",
      "Loss:  0.25020707\n",
      "Loss:  0.25020233\n",
      "Loss:  0.2501976\n",
      "Loss:  0.25019294\n",
      "Loss:  0.2501883\n",
      "Loss:  0.2501836\n",
      "Loss:  0.25017902\n",
      "Loss:  0.25017434\n",
      "Loss:  0.25016978\n",
      "Loss:  0.2501652\n",
      "Loss:  0.25016063\n",
      "Loss:  0.25015607\n",
      "Loss:  0.25015152\n",
      "Loss:  0.25014699\n",
      "Loss:  0.25014248\n",
      "Loss:  0.250138\n",
      "Loss:  0.25013348\n",
      "Loss:  0.25012907\n",
      "Loss:  0.2501246\n",
      "Loss:  0.25012016\n",
      "Loss:  0.25011578\n",
      "Loss:  0.25011134\n",
      "Loss:  0.25010696\n",
      "Loss:  0.25010258\n",
      "Loss:  0.25009823\n",
      "Loss:  0.25009385\n",
      "Loss:  0.25008956\n",
      "Loss:  0.25008526\n",
      "Loss:  0.25008094\n",
      "Loss:  0.25007665\n",
      "Loss:  0.25007236\n",
      "Loss:  0.2500681\n",
      "Loss:  0.25006384\n",
      "Loss:  0.25005957\n",
      "Loss:  0.25005534\n",
      "Loss:  0.25005114\n",
      "Loss:  0.25004697\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "for _ in range(num_epochs):\n",
    "    prediction = model(x) # Forward pass prediction. Saves intermediary values required for backwards pass\n",
    "    loss = loss_func(prediction, t) # Computes the loss for each example, using the loss function defined above\n",
    "    optimizer.zero_grad() # Clears gradients from previous iteration\n",
    "    loss.backward() # Backpropagation of errors through the network\n",
    "    optimizer.step() # Updating weights\n",
    "\n",
    "    #print(\"Prediction: \", prediction.detach().numpy())\n",
    "    print(\"Loss: \", loss.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional optimizer options\n",
    "For faster convergence, the Adam optimizer can be useful. It employs adaptive learning rates specific to each weight. In this exampe you can see that the loss decreases much faster when using Adam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.41995543\n",
      "Loss:  0.2522374\n",
      "Loss:  0.30000538\n",
      "Loss:  0.33381772\n",
      "Loss:  0.3005984\n",
      "Loss:  0.26097837\n",
      "Loss:  0.2492712\n",
      "Loss:  0.2626231\n",
      "Loss:  0.27919346\n",
      "Loss:  0.28276792\n",
      "Loss:  0.2728445\n",
      "Loss:  0.2584318\n",
      "Loss:  0.24892366\n",
      "Loss:  0.24847585\n",
      "Loss:  0.25455806\n",
      "Loss:  0.26072598\n",
      "Loss:  0.2618304\n",
      "Loss:  0.25737414\n",
      "Loss:  0.2507068\n",
      "Loss:  0.24587166\n",
      "Loss:  0.24489293\n",
      "Loss:  0.24693891\n",
      "Loss:  0.24940056\n",
      "Loss:  0.24988283\n",
      "Loss:  0.24768868\n",
      "Loss:  0.24394368\n",
      "Loss:  0.24056448\n",
      "Loss:  0.23890564\n",
      "Loss:  0.23893836\n",
      "Loss:  0.23943803\n",
      "Loss:  0.23899789\n",
      "Loss:  0.23706794\n",
      "Loss:  0.23422623\n",
      "Loss:  0.23158133\n",
      "Loss:  0.22987819\n",
      "Loss:  0.22900188\n",
      "Loss:  0.22818862\n",
      "Loss:  0.22669646\n",
      "Loss:  0.22436437\n",
      "Loss:  0.22164366\n",
      "Loss:  0.21915263\n",
      "Loss:  0.21716228\n",
      "Loss:  0.21542373\n",
      "Loss:  0.21344072\n",
      "Loss:  0.21091351\n",
      "Loss:  0.20796216\n",
      "Loss:  0.20496595\n",
      "Loss:  0.20221032\n",
      "Loss:  0.19966818\n",
      "Loss:  0.19708279\n",
      "Loss:  0.19423817\n",
      "Loss:  0.19115537\n",
      "Loss:  0.1880534\n",
      "Loss:  0.18514189\n",
      "Loss:  0.18245135\n",
      "Loss:  0.17984718\n",
      "Loss:  0.17718841\n",
      "Loss:  0.17446071\n",
      "Loss:  0.17175794\n",
      "Loss:  0.16914925\n",
      "Loss:  0.16657934\n",
      "Loss:  0.1639028\n",
      "Loss:  0.16100776\n",
      "Loss:  0.15789765\n",
      "Loss:  0.15465304\n",
      "Loss:  0.15132636\n",
      "Loss:  0.14788315\n",
      "Loss:  0.14424136\n",
      "Loss:  0.14034921\n",
      "Loss:  0.13620436\n",
      "Loss:  0.13179716\n",
      "Loss:  0.1270575\n",
      "Loss:  0.12188212\n",
      "Loss:  0.116215385\n",
      "Loss:  0.11008188\n",
      "Loss:  0.1035251\n",
      "Loss:  0.09653479\n",
      "Loss:  0.08907728\n",
      "Loss:  0.08119713\n",
      "Loss:  0.07302971\n",
      "Loss:  0.06469795\n",
      "Loss:  0.05627861\n",
      "Loss:  0.047912236\n",
      "Loss:  0.039850745\n",
      "Loss:  0.032326475\n",
      "Loss:  0.02546442\n",
      "Loss:  0.01936293\n",
      "Loss:  0.014117273\n",
      "Loss:  0.009737513\n",
      "Loss:  0.00619113\n",
      "Loss:  0.0034904247\n",
      "Loss:  0.0016194653\n",
      "Loss:  0.00049295306\n",
      "Loss:  3.3239805e-05\n",
      "Loss:  0.00014204928\n",
      "Loss:  0.0006508919\n",
      "Loss:  0.0013984992\n",
      "Loss:  0.0022496092\n",
      "Loss:  0.0030548754\n",
      "Loss:  0.0037007893\n"
     ]
    }
   ],
   "source": [
    "model = Net(n_feature=2, n_hidden=4, n_output=1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.1)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for _ in range(num_epochs):\n",
    "    prediction = model(x)\n",
    "    loss = loss_func(prediction, t)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(\"Loss: \", loss.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and loading models\n",
    "To save the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'filename.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load into a moedl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('filename.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biocontrol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov  4 2022, 08:45:18) [Clang 12.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "b800893415a0c70bc99a913a2794f947c932bde76f0462c9534c0a3d74ebcce5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
